---
title: "Constant-Time Big Numbers: An Introduction"
date: 2021-04-03T11:32:53+02:00
draft: true
katex: true
tags:
  - "Cryptography"
  - "Math"
  - "Security"
---

For the past few months, I've been working on
[a library for constant-time Big Numbers](https://github.com/cronokirby/safenum), in Go.
In this post, I'll try to explain what exactly this work is about, and why you might
care about it. I'm also trying my best to make this introductory post not require
any background in cryptography, but some familiarity with programming would be helpful.

{{<note>}}
This work is being done as my BSc project at EPFL's [DEDIS lab](https://www.epfl.ch/labs/dedis/),
under the supervision of [Professor Bryan Ford](https://people.epfl.ch/bryan.ford), and would
not be possible without their generous help.
{{</note>}}

# What are Big Numbers?

So, as the title suggests, one important concept here is that of "Big Numbers".
Essentially, Big Numbers are nothing more than a way to program with
arbitrarily large integers. Whereas the usual integers you use
in programming have a fixed size, Big Numbers have to be dynamically sized.
Your `uint64` type represents integers from $0$ up to $2^64$, consuming
$64$ bits of memory. A Big Natural Number type would instead be able
to represent all numbers in $\mathbb{N}$. This requires a type that grows
in size as we represent larger numbers.

## Big Numbers are familiar

I'd like to emphasize that Big Numbers are actually a familiar concept.
As programmers, we're used to working with fixed size integers.
This conception is actually somewhat warped, compared to the natural way
you might think of numbers. Most people don't see numbers as artificially
constrained with some upper bound. The numbers seem to go on
forever, essentially:

$$
0, 1, 2, 3, \ldots, 18446744073709551619, \ldots
$$

{{<note>}}
That last number is actually too big to fit over 64 bits, yet it seems
perfectly natural at a first glance.
{{</note>}}

Essentially, a Big Number library provides a way to work with these
arbitrarily large numbers, providing the common operations, like
addition, multiplication, etc.

## Limited hardware size

Computers already have hardware support for arithmetic. CPUs include
operations for working with fixed size integers. The most common
register size is going to be 64 bits nowadays.

Because of this, if you want to work with numbers larger than
64 bits (or whatever your register size happens to be), you'll need
to have an extra software layer implementing this abstraction.

This can't be remedied by increasing the size of a the CPU's registers
either, since Big Number support requires supporting *arbitrarily*
large numbers, in general.

## Schoolbook arithmetic

Not only are Big Numbers familiar to us, but the algorithms for
working with them are as well. In fact, to implement operations
on Big Numbers, you start with the pen-and-paper recipes you learned
back in grade-school.

We usually work with numbers in base 10. This means that we interpret a sequence
of digits, such as:

$$
1234
$$

as the product of each digit with the matching power of 10:

$$
1 \cdot 1000 + 2 \cdot 100 + 3 \cdot 10 + 4 \cdot 1
$$

We could emphasize that we're taking powers of 10 here, writing this
number as:

$$
1 \cdot 10^3 + 2 \cdot 10^2 + 3 \cdot 10^1 + 4 \cdot 10^0
$$

To add two numbers on pen and paper, you first line them up:

TODO ILLUSTRATION

and then you add the digits one by one, making sure to keep track of any
carry generated by an addition:

TODO ILLUSTRATION

A carry happens when two digits add up to something greater than 9. For example,
$5 + 7 = 12$.

Multiplication uses a similar method, where you first multiply one number
by each digit of the other, shifted with the right number of zeros:

TODO ILLUSTRATION

And then, you add all of these terms together:

TODO ILLUSTRATION

Similarly, there are pen-and-paper algorithms to implement all of the operations on big numbers,
and these serve as the foundation for a library programming these up.

## Digits, bits, and limbs

Conceptually, I like thinking of big numbers as being large decimal numbers, like:

$$
43707
$$

We could also express this number in an alternate base. For example, in base 2, this would
be:

$$
1010101010111011
$$

And in base 16, or hexadecimal, this would be:

$$
AABB
$$

In base 10, we usually refer to each individual component of a number as a "digit".
In base 2, these become "bits", and in base 16, I guess you would call these "nibbles"
(being 4 bits long), but I haven't really used that terminology.

In practice, you want to represent your big numbers using a much larger base. Typically,
you'd want each component of your number to make use of the full width of your CPU's
registers. This means you're representing numbers in base $2^64$. With components
this big, we usually call these "limbs": wordplay on "digits", of course.

The foundation for working with Big Numbers (at least in the libraries I know of),
is thus representing them over a list of individual limbs, making a number
over base $2^m$.

# Big Numbers in Cryptography

Big Numbers have a natural application in Computational Mathematics, where you don't
want to be constrained by the size of your machine integers. In fact,
because Big Numbers map more cleanly onto our unconstrained conception of numbers,
some programming languages use them *by default*: Python, namely.

The application that we're interested in here is in *Cryptography*. Specifically,
*public-key Cryptography*. The main public-key cryptosystems are actually based
on the application of Big Numbers; at least conceptually.

## RSA

RSA is perhaps the prototypical example of a public key cryptosystem. I won't be
going over the details of how RSA works; just a few highlights about how it
used Big Numbers.

Essentially, you have a public key $(e, N)$ which consists of an exponent
$e \in \\{0, \ldots, N - 1\\}$ and a natural number $N \in \mathbb{N}$. Operations
are done modulo $N$ later. Your private key is another exponent $d$.

Typically, you talk about $2048$ or $4096$ bit RSA. This refers to the size of the
modulus $N$, in bits. Since you're working with numbers that are about
$2048$ bits long, you need to have support for Big Numbers.

Furthermore, you also need your number library to support *modular arithmetic*, since
the operations making up the RSA system work modulo $N$.

## Elliptic Curves

With Elliptic Curve Cryptography, you have much more complicated operations as
your primitive. When using a prime field, you make this operations using
arithmetic modulo $p$, a prime number. Typically, $p$ is going to be 256 bits or so.
In this case, you can use Big Numbers, along with the modular arithmetic operations.
This only works for prime fields though. For Galois Fields, you need different
operations: multiplication is no longer just modular arithmetic.

One advantage of ECC over RSA is that the parameters you're working with
can be chosen well in advance. The curve you're using is part of the system itself,
whereas the parameters in RSA are chosen dynamically. Because of this, it's possible
to handcode operations over a given curve using exactly the right sized numbers,
whereas RSA requires using dynamically sized Big Numbers.

Because of this, as well as the relative
smallness of the numbers involved,
most implementations of ECC opt for doing things in a curve specific way,
over using a generic Big Number library. The disadvantage is that it requires much
more effort to adopt a new curve, since you need to write a new implementation
as well.

# Timing attacks

So far, we've seen what Big Numbers are, and now it's time to see what
we mean by "Constant-Time". In fact, it's because of the applications
to Cryptography that we care about being Constant-Time in the first place.

Succintly, you can call a program "Constant-Time" when it isn't
vulnerable to "timing attacks". A timing attack, broadly speaking,
is when observing the timing patterns of some program allow an adversary
to violate the desired security properties of that program.

For example, if an adversary can infer the value of some secret
based on observations of the execution time of some program using
that secret, then that would constitute a *timing attack*.

## A toy example

Timing attack in password comparison.

## More subtle sources

Timing leaks through caches, branch prediction.

# Incomplete mitigations

There are a few obvious mitigations, but they aren't perfect.

## Sleeping

People think of trying to sleep a random amount, but a signal is still present.

## Blinding operations

Without going into the mathematics, a certain amount of
randomness can be mixed into some number before a sensitive operation,
then mixed back out after the number.

# Timing leaks in Go

Go's big number library, and how it might not be suitable for crypto.

## Leaky algorithms

Some algorithms are fundamentally leaky.

## Unpadded numbers

Algorithms inevitably leak the length of a number, and Go doesn't allow
hiding this length.

# What does constant-time mean?

What exactly does constant-time mean, when working with arbitrarily large numbers.

## True vs announced length

The difference between the limbs needed to represent a number, and the limbs
we display a number as having.

## Better algorithms

Making sure we don't leak the values through our algorithms

## Leaking modulus sizes

Leaking the modulus's size is desirable for performance, and is
also a reasonable security assumption.

# Some basic techniques

A few of the tricks used to make this happen.

## Some rules of thumb

A basic model of what constraints the code has to operate under.

## Handling choices

How do you replace ifs with bitwise manipulation.

## Looping on announced lengths

Making sure to loop only on the announced lengths

# Safenum

## The basic API approach

## Some implementation notes

## Performance differences

# Why Go?

Why was Go the language of choice for this project.

## In a vacuum

What language would you choose for this project in a vacuum?

## Being useful

What integrations are necessary for this project, why is it desirable
to use Go?

## Some downsides to Go

What are some inconvenient aspects of using Go

# Conclusion

# References
